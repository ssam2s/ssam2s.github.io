<!DOCTYPE HTML>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-MBNHLWD0TG"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'G-MBNHLWD0TG');
    </script>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Jun-Hyeok Seo</title>

    <meta name="author" content="Jun-Hyeok Seo">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
  </head>

  <body>
    <div class="container">
      <header class="header">
        <div class="header-content">
          <div class="header-text">
            <h1 class="name">Jun-Hyeok Seo</h1>
            <p>
              I am second-year M.S. student in <a href="https://ai.knu.ac.kr/">Department of Artificial Intelligence at Kyungpook National University</a>, advised by Prof. Dong-Gyu Lee</a>.
            </p>
            <p>
              Prior to KNU, I received a B.S. degree in <a href="https://ce.kumoh.ac.kr/ce/index.do">Department of Computer Engineering at Kumoh National Institute of Technology</a>, advised by Prof. Hyeonju Yoon</a>.
              I worked on diverse topics in computer vision.
            </p>
            <div class="social-links">
              <a href="https://www.linkedin.com/in/ssam2s/">LinkedIn</a> &nbsp;/&nbsp;
              <a href="mailto:ssam2s@knu.ac.kr">Email</a> &nbsp;/&nbsp;
              <a href="https://www.instagram.com/xeo_xxn">Instagram</a> &nbsp;/&nbsp;
              <a href="https://github.com/ssam2s">Github</a>
            </div>
          </div>
          <div class="header-image">
            <a href="images/jhseo.jpg">
              <img src="images/jhseo.jpg" alt="profile photo" class="profile-photo">
            </a>
          </div>
        </div>
      </header>

      <section class="section">
        <h2>News</h2>
        <ul class="news-list">
          <li>08/2025: My paper has been accepted in KIISE Trans. on Computing Practice (KCI).</li>
          <li>05/2025: My paper has been accepted in <a href="https://vizwiz.org/tasks-and-datasets/image-classification/">CVPRW 2025</a>.</li>
          <li>05/2025: 3rd Place Winner of VizWiz Zero-Shot Image Classification Grand Challenge in <a href="https://vizwiz.org/tasks-and-datasets/image-classification/">CVPR 2025</a>.</li>
          <li>02/2025: Received a Best Presentation Paper Award in KSC 2024.</li>
          <li>12/2024: My paper has been accepted in Korea Software Congress 2024(<a href="https://www.kiise.or.kr/conference/KSC/2024/">KSC 2024</a>).</li>
          <li>08/2024: Won a top prize(The Mayor of Daegu Metropolitan City Award) of the <a href="https://n.news.naver.com/article/030/0003228170?sid=102">ÎåÄÍµ¨Í¥ëÏó≠Ïãú Í≥µÍ≥µÎç∞Ïù¥ÌÑ∞ ÌôúÏö© Ï∞ΩÏóÖÍ≤ΩÏßÑÎåÄÌöå</a>.</li>
          <li>03/2024: Joined <a href="https://sites.google.com/view/cvmilab">Computer Vision and Machine Intelligence Lab.</a> at KNU(Kyungpook National University).</li>
        </ul>
      </section>

      <section class="section">
        <h2>Education & Affiliations</h2>
        <div class="education-list">
          <div class="education-item">
            <img src="data/knu.svg" alt="KNU Logo" class="institution-logo">
            <div class="education-details">
              <h3>Integrated M.S.-Ph.D. in Artificial Intelligence</h3>
              <p>Advised by Professor Dong-Gyu Lee.</p>
              <p><em>Mar 2024 - Present | Daegu, Korea</em></p>
            </div>
          </div>
          <div class="education-item">
            <img src="data/kit.svg" alt="KIT Logo" class="institution-logo">
            <div class="education-details">
              <h3>B.S. in Computer Engineering</h3>
              <p>Advised by Professor Hyeonju Yoon.</p>
              <p><em>Mar 2020 - Feb 2024 | Gumi, Korea</em></p>
            </div>
          </div>
        </div>
      </section>

      <section class="section">
        <h2>Research</h2>
        <p>My research interest is human-centric AI, especially in the field of computer vision (CV).</p>

        <div class="research-list">
          <div class="research-item">
            <div class="research-image">
                <img src='images/Nira.png' width="160" height="90">
            </div>
            <div class="research-content">
              <a href="https://arxiv.org/">
                <span class="papertitle">NiRA: Noise-invariant Causal Discrete Representation Alignment for Video-based Remote Physiological Measurement</span>  
              </a>
              <br>
                <strong>Jun-Hyeok Seo</strong>, Dong-Gyu Lee
              <br>
              <span class="tag purple">AAAI, 2026</span>
              <br>
              <p></p>
              <p>
                Under Review
              </p>
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
                <img src='images/KAIC2025.png' width="160" height="90">
            </div>
            <div class="research-content">
              <a href="data/2025KAIC.pdf">
                <span class="papertitle">Context-Aware Adaptive Mask Selection for Consistent Referring Video Object Segmentation</span>  
              </a>
              <br>
                <strong>Jun-Hyeok Seo</strong>, Young-Woo Youn, Wonjun Choi, Dong-Gyu Lee
              <br>
              <span class="tag gray">KAIC, 2025</span>
              <br>
              <p></p>
              <p>
                This paper proposes a context-aware adaptive mask selection framework for referring video object segmentation (RVOS). The method integrates CLIP-based keyframe selection, mask consistency scoring, text-guided object number determination, and temporal mask interpolation to ensure both temporal stability and accurate multi-object segmentation.
              </p>
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
                <img src='images/Diffusion2.png' width="160" height="90">
            </div>
            <div class="research-content">
              <a href="data/Diffusion2.pdf">
                <span class="papertitle">Enhancing Detail Quality in Latent Diffusion Model-Based Super-Resolution via Multi-Scale Discrete Wavelet Transform</span>  
              </a>
              <br>
                <strong>Jun-Hyeok Seo</strong>, Dong-Gyu Lee
              <br>
              <span class="tag red">KTCP, 2025</span>
              <br>
              <p></p>
              <p>
                This paper proposes a multi-scale discrete wavelet transform framework designed to enhance the detail quality of super-resolution (SR) methods based on latent diffusion model. Experimental results on benchmark datasets demonstrate that the proposed framework outperforms previous methods, effectively improving the detail quality of super-resolved images.
              </p>
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
                <img src='images/cvprw2025.png' width="160" height="80">
            </div>
            <div class="research-content">
              <a href="data/CVPRw2025.pdf">
                <span class="papertitle">Multi-Perspective LVLM Prompting for Robust Zero-Shot Image Classification</span>  
              </a>
              <br>
                Wonjun Choi, Jeong-Cheol Lee, <strong>Jun-Hyeok Seo</strong>, Dong-Gyu Lee
              <br>
              <span class="tag blue">CVPRw, 2025</span>
              <br>
              <p></p>
              <p>
                This paper proposes a multi-perspective prompt-based framework for zero-shot image classification using LVLMs, enhancing robustness by integrating object-aware, intent-aware, and reason-aware prompts, and achieving superior performance through ensemble prediction on the VizWiz dataset.
              </p>
            </div>
          </div>

          <div class="research-item">
            <div class="research-image">
              <img src="images/Diffusion.png" width="160" height="160">
            </div>
            <div class="research-content">
              <a href="data/Diffusion.pdf">
                <span class="papertitle">Diffusion Model Based Image Super-Resolution with Multi-Scale High Frequency Error Maps</span>
              </a>
              <br>
              <strong>Jun-Hyeok Seo</strong>, Dong-Gyu Lee
              <br>
              <span class="tag gray">KSC, 2024</span><span class="tag purple">Best Presentation Paper Award</span>
              <p>
                This paper proposes a diffusion model-based image super-resolution framework using multi-scale high-frequency error maps, enabling accurate restoration of image details and outperforming existing methods on benchmark datasets.
              </p>
            </div>
          </div>
      
          <!-- TrOCR KIIT 2023 -->
          <div class="research-item">
            <div class="research-image">
              <img src="images/TrOCR.png" width="160" height="160">
            </div>
            <div class="research-content">
              <a href="data/TrOCR.pdf">
                <span class="papertitle">Robust Captcha Image Recognition Algorithm Using Transformer-Based OCR Model</span>
              </a>
              <br>
              <strong>Junhyeok Seo</strong>, Seongjin Jang, Minjun Kim, Hyeonju Yoon
              <br>
              <span class="tag gray">KIIT, 2023</span><span class="tag bronze">Bronze Award</span>
              <p>
                This paper proposes a robust OCR algorithm using the end-to-end Transformer-based TrOCR model, achieving a Character Error Rate (CER) of approximately 0.08 and significantly outperforming Tesseract OCR and EasyOCR.
              </p>
            </div>
          </div>
      
          <!-- CarSeg KCC 2023 -->
          <div class="research-item">
            <div class="research-image">
              <img src="images/CarSeg.png" width="160" height="160">
            </div>
            <div class="research-content">
              <a href="data/CarSeg.pdf">
                <span class="papertitle">Deep Learning-Based Vehicle Damage Area Detection System using Semantic Segmentation</span>
              </a>
              <br>
              <strong>Junhyeok Seo</strong>, Hyeonju Yoon
              <br>
              <span class="tag gray">KCC, 2023</span>
              <p>
                This paper proposes a system for detecting vehicle damage areas through semantic segmentation using the UNet++ model, achieving pixel-level accuracy with an average IoU of 0.94 and comparing its performance with the conventional U-Net model.
              </p>
            </div>
          </div>
      
          <!-- StainSeg KIIT 2023 -->
          <div class="research-item">
            <div class="research-image">
              <img src="images/StainSeg.png" width="160" height="160">
            </div>
            <div class="research-content">
              <a href="data/StainSeg.pdf">
                <span class="papertitle">Segmentation of Stain Area in Clothing Images using UNet++</span>
              </a>
              <br>
              <strong>Junhyeok Seo</strong>, Gunwook Kim, Junsu Park, Sungyoung Kim
              <br>
              <span class="tag gray">KIIT, 2023</span><span class="tag bronze">Bronze Award</span>
              <p>
                This paper proposes a deep learning-based technique for detecting stains in clothing images using a UNet++ model with an EfficientNet encoder pre-trained on ImageNet, achieving an average IoU of approximately 0.9589.
              </p>
            </div>
          </div>
      
          <!-- DomainDict KIIT 2022 -->
          <div class="research-item">
            <div class="research-image">
              <img src="images/DomainDict.png" width="160" height="160">
            </div>
            <div class="research-content">
              <a href="data/DomainDict.pdf">
                <span class="papertitle">Domain Dictionary Construction for Automatic Analysis of Subject CQI Reports</span>
              </a>
              <br>
              Gunwoo Kim, Junsu Park, <strong>Junhyeok Seo</strong>, Yuchul Jung, Hyeonju Yoon
              <br>
              <span class="tag gray">KIIT, 2022</span><span class="tag silver">Silver Award</span>
              <p>
                This paper analyzes various words used in CQI reports and constructs a domain dictionary to provide the foundation for future automatic analysis of such reports.
              </p>
            </div>
          </div>
      
          <!-- Vehicle Detection System KIIT 2022 -->
          <div class="research-item">
            <div class="research-image">
              <img src="images/Vehicle Detection System.png" width="160" height="160">
            </div>
            <div class="research-content">
              <a href="data/Vehicle Detection System.pdf">
                <span class="papertitle">Deep Learning-Based Vehicle Model and License Plate Identification System using Vehicle Image</span>
              </a>
              <br>
              <strong>Junhyeok Seo</strong>, Hyeonju Yoon
              <br>
              <span class="tag gray">KIIT, 2022</span><span class="tag orange">Gold Award</span>
              <p>
                This paper proposes a vehicle model recognition and license plate identification system using YOLOv5, enabling both tasks simultaneously and reducing computational cost compared to previous methods.
              </p>
            </div>
          </div>
          
        </div>
      </section>

      <!-- Achievements Section -->
      <section class="section">
        <h2>Achievements</h2>
        <ul class="awards-list">
          <li>[Scholarship] Merit Scholarship(IH003) from KNU (2024-2)</li>
          <li>[Scholarship] Merit Scholarship(IH003) from KNU (2024-1)</li>
          <li>[Awards] 3rd Place Winner of VizWiz Zero-Shot Image Classification Grand Challenge in CVPR 2025 (2025)</li>
          <li>[Awards] The Mayor of Daegu Metropolitan City Award of the 
            <a href="https://n.news.naver.com/article/030/0003228170?sid=102">ÎåÄÍµ¨Í¥ëÏó≠Ïãú Í≥µÍ≥µÎç∞Ïù¥ÌÑ∞ ÌôúÏö© Ï∞ΩÏóÖÍ≤ΩÏßÑÎåÄÌöå</a> (2024)
          </li>
          <li>[Awards] Best Presentation Paper Award, Korea Software Congress (2024)</li>
          <li>[Awards] Bronze Award for Outstanding Paper, Proceedings of KIIT Conference (2023)</li>
          <li>[Awards] Bronze Award for Outstanding Paper, Proceedings of KIIT Conference (2023)</li>
          <li>[Awards] Silver Award for Outstanding Paper, Proceedings of KIIT Conference (2022)</li>
          <li>[Awards] Gold Award for Outstanding Paper, Proceedings of KIIT Conference (2022)</li>
        </ul>
      </section>
      
      
      <!-- Teaching Assistant Section -->
      <section class="section">
        <h2>Teaching Assistant</h2>
        <ul class="services-list">
          <li>(Undergraduate) Data Analysis (CAIB226) (2025-1)</li>
          <li>(Undergraduate) Problem-Solving Skills and Algorithms (CAIB0230) (2024-2)</li>
        </ul>
      </section>
      
      
      <!-- Professional Experience Section -->
      <section class="section">
        <h2>Professional Experience</h2>
        <div class="experience-list">
      
          <div class="experience-item">
            <img src="images/Í≤ΩÎ∂ÅÎåÄ.png" alt="KNU Logo" class="institution-logo">
            <div class="experience-details">
              <h3>Kyungpook National University</h3>
              <p>Undergraduate Tutor</p>
              <p><em>Mar 2025 - Jul 2025 | Daegu, Korea</em></p>
            </div>
          </div>
      
          <div class="experience-item">
            <img src="images/LG.png" alt="LG Aimers Logo" class="institution-logo">
            <div class="experience-details">
              <h3>LG Aimers</h3>
              <p>Education Completion</p>
              <p><em>Jul 2022 - Sep 2022 | Online</em></p>
            </div>
          </div>
      
          <div class="experience-item">
            <img src="images/ÌÉÄÏûÑÏª¥Ï¶à.png" alt="TimeComs Logo" class="institution-logo">
            <div class="experience-details">
              <h3>TimeComs</h3>
              <p>Web Publishing Team</p>
              <p><em>Jul 2022 - Sep 2022 | Gumi, Korea</em></p>
            </div>
          </div>
      
          <div class="experience-item">
            <img src="images/ÏõêÏùµ.png" alt="Wonik QnC Logo" class="institution-logo">
            <div class="experience-details">
              <h3>WONIK QnC</h3>
              <p>Facilities Management Department - Data Processing</p>
              <p><em>Jul 2021 - Sep 2021 | Gumi, Korea</em></p>
            </div>
          </div>
      
          <div class="experience-item">
            <img src="images/Í∏àÏò§Í≥µÎåÄ.png" alt="KIT Logo" class="institution-logo">
            <div class="experience-details">
              <h3>Kumoh National Institute of Technology</h3>
              <p>Freshmen Mentor</p>
              <p><em>Mar 2021 - Feb 2022 | Gumi, Korea</em></p>
            </div>
          </div>
      
        </div>
      </section>
                
          



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Source code credit to <a href="https://jonbarron.info/" target="_blank">Dr. Jon Barron</a></p>
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
    <table style="width:50%;margin-left:auto;margin-right:auto;">
      <tbody>
        <tr>
          <td>
            <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=b5b5b5&w=340&t=tt&d=v8W-Ch_-iX1wT7o_8KQ5gQB6p4WCNY7aTSkwcFpQmZE&co=ffffff&ct=000000'></script>
          </td>
        </tr>
      </tbody>
    </table>
  </body>
</html>
